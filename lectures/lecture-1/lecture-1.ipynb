{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 1:  Basic concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What do we plan to cover\n",
    "\n",
    "Deep learning is changing every day now.  So, this is the today list of topics for this course:\n",
    "\n",
    "- Convolution & Co. Classical CNNs and how do they work\n",
    "- The pecularities of training deep neural networks (momentum, SGD, loss surfaces, the mystery of generalization).\n",
    "- CV tasks: object detection, semantic segmentation, transfer learning\n",
    "- Learning from sequential data: from RNN to Transformer\n",
    "- Transformer architecture: from NLP back to CV again\n",
    "- Graph neural networks\n",
    "- General tricks for efficient training: initialization, data augmentation, compression and pruning, clipping, multi-precision setting, distributed training primitives\n",
    "- Training of large deep models: checkpointing, offloading, efficient communications, low-precision training.\n",
    "- Self-supervised learning\n",
    "- Zero/one-shot learning\n",
    "- Adversarial attacks and adversarial training.\n",
    "- Generative models I: Autoregressive models\n",
    "- Generative models II: Variational autoencoders\n",
    "- Generative models III: Normalizing flows, NeuralODEs.\n",
    "- Generative models IV: GAN and score matching.\n",
    "- Generative models V: Diffusion models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Homeworks, etc.\n",
    "\n",
    "We plan to have three homeworks + a project.\n",
    "\n",
    "Nothing more! \n",
    "\n",
    "Now it is a 3-credit course.\n",
    "\n",
    "**Idea:** Cover 'basic minimum', for more detailed information -> next year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Todays lecture: Basic concepts\n",
    "- **General discussion**: how deep learning is different from classical machine learning\n",
    "- **Supervised/unsupervised learning**: abstract formulation\n",
    "- **Fully connected neural networks**: What it is and why depth matters\n",
    "- **The concept of backpropagation and 'cheap gradients'**: why it is important to compute the gradient in a fast way (and how)\n",
    "- **Convolutional neural networks**: Brief definition of the CNN\n",
    "- **Popular deep learning libraries**: Tensorflow, Pytorch and Jax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How deep learning is different from classical machine learning \n",
    "\n",
    "\n",
    "There is subtle difference between **machine learning** / **deep learning** / **'artificial intelligence'**.\n",
    "\n",
    "These terms are now used interchangeably.\n",
    "\n",
    "Lets try to make some formal definitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine learning\n",
    "\n",
    "**Machine learning** -\n",
    "algorithms that parse data, learn models using these data and make informed decisions or predictions.\n",
    "\n",
    "It includes linear classifiers, random forests, etc. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep learning\n",
    "\n",
    "The term **Deep learning** appeared around 1986 (not in the context of neural networks)\n",
    "\n",
    "and became common around mid-2000.\n",
    "\n",
    "The first influencial deep learning paper is typically considered to be the paper by Hinton and Salakhutdinov.\n",
    "\n",
    "It referred mostly to deep neural network models, now used in a broader sense.\n",
    "\n",
    "The main idea: **hierachical derivation** of features for learning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Awfully incomplete history\n",
    "\n",
    "- **1943**: First artificial neuron by McCulloch and Pitts.\n",
    "- **1958**: Single-layer perceptron: 1958 Rosenblatt perceptron\n",
    "- **1969**: Minsky 'Perceptrons' showed that single-layer has limited capacity (leading to decline in using neural networks).\n",
    "- **1971**: Group method of data handling (Ivakhnenko and Lapa), polynomial models\n",
    "- **1974**: backpropagation (Werbos)\n",
    "- End of **1980s**: LeCun (CNN), RNN\n",
    "- **1990s**: [LSTM by Schmidhuber](https://proceedings.neurips.cc/paper/1996/file/a4d2f0d23dcc84ce983ff9157f8b7f88-Paper.pdf).\n",
    "- **2012**: ImageNet dataset, [Krizhevsky et. al](https://dl.acm.org/doi/pdf/10.1145/3065386) show superiority of deep convolutional neural network models\n",
    "- **2014**: [Adam](https://arxiv.org/pdf/1412.6980.pdf%5D), [GAN](https://arxiv.org/pdf/1406.2661.pdf), [attention](https://arxiv.org/pdf/1409.0473.pdf)\n",
    "- **2015**: [Batchnorm](http://proceedings.mlr.press/v37/ioffe15.pdf), [ResNet](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n",
    "- **2017**: [Transformer](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "- **2018**: GPT, [BERT](https://arxiv.org/abs/1810.04805), ...\n",
    "- **2020**: [Vision transformers](https://arxiv.org/abs/2010.11929), diffusion models\n",
    "- **2021**: Self-supervised ([CLIP](https://arxiv.org/pdf/2103.00020.pdf)) \n",
    "- **2022-** ChatGPT, GPT-4\n",
    "- **2022-** Flow Matching, Multi-modality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised learning\n",
    "\n",
    "\n",
    "The simplest machine learning model is **supervised learning** (which you probably already heard about).\n",
    "\n",
    "You have:\n",
    "1. A training set $X^{\\mathrm{train}} = \\{ (x_i, y_i), i=1,\\ldots, N$ \\}, and a test set  $X^{\\mathrm{test}} = \\{ (\\tilde{x}_j, \\tilde{y}_j), j=1,\\ldots, M$ \\},\n",
    "2. A loss function $l(y, \\hat{y})$ that measures how accurate is the prediction $\\hat{y}$ for a particular sample.\n",
    "3. A machine learning model $\\hat{y} = f(x, \\theta)$, defined by certain parameters $\\theta$\n",
    "\n",
    "This can be regression, binary classification or multiclass classification in the simplest cases.\n",
    "\n",
    "The vectors $x_i$ are called features of the object. Feature engineering and selection of the model class is one of the crucial tasks of an ML/DS expert.\n",
    "\n",
    "We want the model to **generalize**: good train accuracy, and good test accuracy. This is achieved by:\n",
    "\n",
    "- Train/test splitting (no difference between train test, sampled i.i.d)\n",
    "- Model architecture (should be appropriate for the task)\n",
    "- Optimization method (should avoid 'badly generalizable' minima)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Abstract problem formulation\n",
    "\n",
    "Thus, in supervised learning our goal is to minimize the **loss function**\n",
    "\n",
    "$$g(\\theta) \\rightarrow \\min, \\quad g(\\theta) = \\frac{1}{N} \\sum_{i=1}^N l(y_i, \\hat{y}_i), \\quad \\hat{y}_i = f(x_i, \\theta).$$\n",
    "\n",
    "Looks like a general optimization problem.\n",
    "\n",
    "**Question**: Does this optimization problem have any special structure, which we can use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss functions\n",
    "\n",
    "Thus, in supervised learning our goal is to minimize the **loss function**\n",
    "\n",
    "$$g(\\theta) \\rightarrow \\min, \\quad g(\\theta) = \\frac{1}{N} \\sum_{i=1}^N l(y_i, \\hat{y}_i), \\quad \\hat{y}_i = f(x_i, \\theta).$$\n",
    "\n",
    "Possible choices of the loss function:\n",
    "\n",
    "1. **Mean squared error (MSE)**: \n",
    "\n",
    "$$l(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2$$\n",
    "\n",
    "2. For binary classification, we can use **binary cross entropy** (we assume that the output $\\hat{y} \\in [0, 1]$).\n",
    "\n",
    "$$l(y, \\hat{y}) = -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y})$$\n",
    "\n",
    "3. For multiclass classification we can use **cross entropy**:\n",
    "\n",
    "$$l(y, \\hat{y}) = -\\sum_{c=1}^Cy_c\\log(\\hat{y}_c)$$\n",
    "\n",
    "where $y$ is a one-hot encoded vector of true labels, $\\hat{y}$ is the predicted class probabilities, and $C$ is the number of classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised learning\n",
    "\n",
    "The goal of unsupervised learning is to extract patterns from the data without exact labels to predict.\n",
    "\n",
    "We have to invent **loss functions** for different tasks.\n",
    "\n",
    "One of the most common settings is to model the probability distribution $p(x)$ of the observed data, \n",
    "\n",
    "and maximize the likelihood\n",
    "\n",
    "$$g(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\log p(x_i, \\theta).$$\n",
    "\n",
    "Other type of unsupervised learning include **constrastive learning**, **clustering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep learning architectures\n",
    "\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <div style=\"flex: 1; margin-right: 10px; margin-top: 20px; width: 100%\">  \n",
    "The parametrization of the model $f(x, \\theta)$ is the key ingredient.\n",
    "\n",
    "The standard approach of 'classical' machine learning is to design a certain **map** $E(x)$ with comes from the expert knowledge.\n",
    "\n",
    "Then, learn a simple (i.e., linear) model \n",
    "\n",
    "$$y = f(E(x), \\theta)$$. \n",
    "\n",
    "The concept of **end-to-end learning** is to avoid predesigned feature engineering, rather than consider **parametric building blocks**, and learn them simultaneously.\n",
    "\n",
    "The simplest representation of this form is **feedforward neural networks**.\n",
    "    </div>\n",
    "    <div style=\"flex: 1; margin-top: 20px\">\n",
    "        <img src=\"classic-vs-deep.png\" width=\"100%\">\n",
    "    </div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fully connected neural networks \n",
    "\n",
    "The basic deep learning model is the **feedforward model neural network** (also known as multilayer perceptron, MLP).\n",
    "\n",
    "Mathematically, it is a superposition of **linear** and simple **non-linear** transformations:\n",
    "\n",
    "$$f(x) = f_k(f_{k-1}(\\ldots (f_0(x))),$$\n",
    "\n",
    "where\n",
    "\n",
    "$$f_k(x_k) = \\sigma( W_k x_k + b_k)$$ is one layer of the transformation.\n",
    "\n",
    "$W_k$ is an $r_{k} \\times r_{k-1}$ matrix (called **weight**), $b_k$ is a vector of length $r_k$ called **bias**,\n",
    "\n",
    "$\\sigma$ is a pointwise nonlinearity function (sigmoid, or ReLU).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Popular non-linearities\n",
    "\n",
    "1. **Sigmoid:** The sigmoid function maps any real-valued number to a value between 0 and 1, making it useful for binary classification problems. It is defined as:\n",
    "\n",
    "   $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "2. **ReLU (Rectified Linear Unit):** The ReLU activation function is defined as the positive part of its argument, i.e., $f(x) = \\max(0, x)$. It has become popular because of its simplicity and effectiveness in deep neural networks.\n",
    "\n",
    "3. **Leaky ReLU:** The Leaky ReLU is similar to ReLU, but allows for a small, non-zero gradient when the input is negative. It is defined as:\n",
    "\n",
    "   $$f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ ax, & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "   where $a$ is a small positive constant.\n",
    "\n",
    "4. **Tanh (Hyperbolic Tangent):** The tanh function maps any real-valued number to a value between -1 and 1. It is defined as:\n",
    "\n",
    "   $$\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$\n",
    "\n",
    "5. **Sine:** The sine function can be used as an activation function, especially in audio-related tasks. It is defined as:\n",
    "\n",
    "   $$\\sin(x)$$\n",
    "\n",
    "6. **SELU (Scaled Exponential Linear Unit):** The SELU activation function is a self-normalizing activation function that outputs zero mean and unit variance activations, helping to reduce the internal covariate shift problem. It is defined as:\n",
    "\n",
    "   $$f(x) = \\lambda \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha(e^x - 1), & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "   where $\\alpha$ and $\\lambda$ are constants determined from the mean and variance of the input data.\n",
    "\n",
    "7. **Swish:** The Swish activation function is defined as\n",
    "\n",
    "   $$f(x) = x\\sigma(\\beta x)$$\n",
    "\n",
    "   where $\\sigma$ is the sigmoid function and $\\beta$ is a hyperparameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What MLP can approximate: Depth\n",
    "\n",
    "Why do we need deep networks? \n",
    "\n",
    "This is the concept of **expressive power**: deep networks reprsent much broader class of functions.\n",
    "\n",
    "\n",
    "**Meta-theorem**: If we have a random feed-forward deep network, in order to represent it exactly with a shallow network we need exponentially large **width**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "## Building neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Define the model architecture\n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, output_size),\n",
    ")\n",
    "\n",
    "x0 = torch.linspace(-1, 1, 256)\n",
    "print(x0.shape)\n",
    "#y = model(x0) #Will be an error, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What MLP can approximate\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <div style=\"flex: 1; margin-right: 20px; margin-top: 20px; width: 100%\">  \n",
    "There is a famous Universal Approximation Theorem by <a href=\"https://web.njit.edu/~usman/courses/cs675_fall18/10.1.1.441.7873.pdf\">Cybenko (1989)</a> and <a href=\"https://www.sciencedirect.com/science/article/abs/pii/0893608089900208\"> Hornik et al. (1989)</a>.\n",
    "        \n",
    "        \n",
    "Let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a continuous function. Then $f$ can be approximated arbitrarily well by a neural network with a single hidden layer, if and only if the activation function of the network is a continuous, non-constant, and bounded function that is not a polynomial.\n",
    "    </div>\n",
    "    <div style=\"flex: 1; margin-top: 20px\">\n",
    "        <img src=\"nn1-1.png\" width=\"100%\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep networks\n",
    "\n",
    "The problem of universal approximation theorem is that the width can be **exponentially large** for good approximation.\n",
    "\n",
    "Can depth help for the approximation of simple functions?\n",
    "\n",
    "- **Theoretically**: Yes (see the [pionering work](https://arxiv.org/pdf/1610.01145.pdf) by Dmitry Yarotsky)\n",
    "- **Practically**: For regression depth does not help at all.\n",
    "\n",
    "For image processing/Natural Language Processing (NLP) the depth of deep neural network helps a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The concept of backpropagation and 'cheap gradients' \n",
    "\n",
    "\n",
    "The most straightforward optimization method will be **gradient descent**.\n",
    "\n",
    "$$g(\\theta) \\rightarrow \\min, \\quad g(\\theta) = \\frac{1}{N} \\sum_{i=1}^N l(y_i, \\hat{y}_i), \\quad \\hat{y}_i = f(x_i, \\theta).$$\n",
    "\n",
    "$$\\theta_{k+1} = \\theta_k - \\alpha \\nabla_{\\theta} g. $$\n",
    "\n",
    "Parameter $\\alpha$ is called **learning rate** for training, it also may depend on the step $\\alpha = \\alpha_k$ (this will be known as **learning rate schedule**).\n",
    "\n",
    "\n",
    "Note, that it was not the first idea to use gradient descent for feed-forward networks! \n",
    "\n",
    "Such things as sequential learning, or second-order methods have been tried many times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to compute the gradient in a cheap way? \n",
    "\n",
    "Given $$ g(\\theta) = \\frac{1}{N} \\sum_{i=1}^N l(y_i, \\hat{y}_i), \\quad \\hat{y}_i = f(x_i, \\theta).$$ how to compute the gradient in a cheaper way? \n",
    "\n",
    "I need at least two answers! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recipe to speed up 1: Stochastic gradient descent\n",
    "\n",
    "Instead of summing over all dataset, randomly pick a subset (called **batch**) and evaluate the gradient on it:\n",
    "\n",
    "$$\\nabla g(\\theta) \\approx \\frac{1}{B} \\sum_{k=1}^B \\nabla_{\\theta} l(y_{i_k}, \\hat{y}_{i_k}), \\quad \\hat{y}_{i_k} = f(x_{i_k}, \\theta).$$\n",
    "\n",
    "**Obvious**:  The complexity of a gradient is reduced by a factor of $\\frac{N}{B}$.\n",
    "\n",
    "**Not obvious**: Stochastic gradient descent (SGD) leads to completely different convergence than ordinary gradient descent.\n",
    "\n",
    "But still, how to compute the gradient for a single sample of the dataset?\n",
    "\n",
    "A good deep learning model may have hunderds of millions of parameters even if it is small..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recipe to speed up 2: Computing the gradient of a function\n",
    "\n",
    "Our function is given as a superposition of simple functions. \n",
    "\n",
    "It depends on many parameters. \n",
    "\n",
    "Does the complexity of the evaluation of the gradient depends on the number of variables? \n",
    "\n",
    "(I.e. if the model has $P$ parameters, we need to compute $P$ values).\n",
    "\n",
    "**[Baur-Strassen theorem](https://pdf.sciencedirectassets.com/271538/1-s2.0-S0304397500X03179/1-s2.0-030439758390110X/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEEUaCXVzLWVhc3QtMSJIMEYCIQDC1fS05%2BR4R4nUf579k4ZFsW1453%2BtmX4FfRPwcGM3ZgIhAPMO%2Biq9nnYrmiGkTS0t2rrP%2BbmfDuQ5JQYZjYTgAjnuKrMFCB4QBRoMMDU5MDAzNTQ2ODY1Igx2lJUVZeg6AjyT6fMqkAXfkYIVa5fOcTRDk8LMukgWf%2FGf0ho4%2FB55wXJhOtNtmXqZyfGwBWOHr19RuQrPOKC3QYzgw%2BiKLOiUah4dI0%2B8A8P7%2BOfBUSMEVw1Gfkr%2BKDjqRYf8H7QBYpKPhXhhNhQYgo%2F7zQtTQ3TOofmlYA1hVk61OdXVm2DaZ0fMqHR3jA3p%2F9sjoLQKaR6sjSebHOJFysS4FUG5Zu1xGBoInrEsW7yj9G%2BPleDyAyznyPo6OBDtFfCj%2B0npyEhF35sHZRCepUee5OXIFH3bHMhrm0YlFMnrkkyRYd0d%2BJsH%2FM5ez3fcEqHoWI0bCwHT0%2FhtYwP4FtjVZyn%2Fd7X%2BD99ofkq4lVcIydktCNfZMr5pcJMCpekXmAfSGdH%2FZ3RocIsthlQ57oAKmCwGDEHCV8z0FPNQnneUR4P1mHnV3VZ%2BpTRFQ5CvR45U7ip6vxAKvLGd1%2BscTphEnSPbOpTp4WQE1KuQgYXDOUw4F%2FDZf2jB%2FaGs37mFJhSzP0NmfqqDDYi%2Ffj6i4zN7YZgSs2oBdXr0UU54U9RfbWso1VTW9TP1KaKBDvGr%2FyhIL4lrUbRiPuP2dpzaIkRysq9IPReU6f4N%2FY%2BNisXKPPCgGhL9Eyz2UPz9lZqZmr9nPfnQ6UkprnC5GZ8gqLWPeDf9bf4Vb95ervaDNgEMdXykxWNgvVEPVKEJjfDM8o5wk6i%2Fjvk6deAz0n%2FmwFad1h4whZG578zy9ID0rJlnMGXSl%2F8WrDy6EPb4F0XLvCu38oO2eT3KQDguo%2FnIkEpJwmbxnc4JqwCXu%2F1lrCFPF4eOv8nbOz93OvjAboO5f5MbJP3dur6cRyS4wJQHEhIDf4AM%2BuGKMeprz0VgimW4pyPYv54W70pXGVaFdDCd4oKhBjqwAaHPaZRh%2BeK1Fm4TETwO%2F%2BC6E%2FBhxEaucJG%2FqSMNP3WM3S37AlttMdsSG3Aks4xP5ZkfS3ziWyrD9fX1bL9c6ePW48LnMUfvKgPpBkLVi9wCC4GaHTSyEsXO0qlf1xgzc60sQWHXPlw1Ab%2FWZNvrwHxMrXdFs9Uki7NdQiNajj7Ynj5p047Edrwjega9%2F2HoPJrcObuVkvtYglG2ryxlwql4iJLP1dkKsCnRt7Qy4Ezi&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230326T220343Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYV46SCF5Y%2F20230326%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=65d2b64eef8507f0d46cf09a9c9a0d248b611fc08b481ee926c1951797fb9b78&hash=a09047e53b176ccc8f1a09bf52156966eda040cffde7bb11579c4a99a6b8eee6&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=030439758390110X&tid=spdf-34475ea8-c7d4-4fd3-b3e9-080281ffdb55&sid=196592cf4892624cf48b6092b520278c0205gxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=1b125506005c0202585e5e&rr=7ae2af699d6292a1&cc=at)** (1983) says (informally) that for almost any computer code:\n",
    "\n",
    "If $g(\\theta)$ takes $K$ flops to evaluate, we can generate an algorithm that compute $\\nabla_{\\theta} g$ in $cK$ flops, where $c$ is a constant.\n",
    "\n",
    "**Downside:** We may need a lot of additional memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation: fast evaluation of the gradient\n",
    "\n",
    "We can use chain rule for feedforward models:\n",
    "\n",
    "$$l(y, f(x, \\theta)) = l(y, f_2(f_1(x, \\theta_1), \\theta_2)),$$\n",
    "\n",
    "How we compute the gradient with respect to $\\theta_2$? \n",
    "\n",
    "$$\\frac{\\partial g}{\\partial \\theta_2} = \\left(\\frac{\\partial f_2}{\\partial \\theta_2}\\right)^{\\top}\\frac{\\partial l}{\\partial \\hat{y}}$$\n",
    "\n",
    "With respect to $\\theta_1$ we need to compute the gradient of $f_2$ with respect to the input.\n",
    "\n",
    "I.e., we first compute the vector $v_2 = \\frac{\\partial l}{\\partial \\hat{y}}$.\n",
    "\n",
    "Then we compute \n",
    "\n",
    "$$v_1 = \\left(\\frac{\\partial f_2}{\\partial x_2}\\right)^{\\top} v_2 $$\n",
    "\n",
    "Then we compute \n",
    "\n",
    "$$\\frac{\\partial g}{\\partial \\theta_1} = \\left(\\frac{\\partial f_1}{\\partial \\theta_1}\\right)^{\\top} v_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reverse mode differentiation\n",
    "\n",
    "- PyTorch and Jax by default use **reverse mode** differentiation, which we just described (i.e. for each computational block we need to implement transposed Jacobian-by-vector product). \n",
    "\n",
    "\n",
    "- There is also a **forward mode** differentiation. \n",
    "\n",
    "- In forward mode, we compute the derivatives **simultaneously** with the value. \n",
    "\n",
    "- Reverse mode and forward mode are two extreme ways of traversing the chain rule for the derivatives.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Idea of forward mode differentiation using dual numbers\n",
    "\n",
    "We introduce **dual numbers**. A dual number has the form $x + \\varepsilon x'$ and $\\varepsilon^2 = 0.$\n",
    "\n",
    "The connection to differentiation can be seen from formal Taylor series:\n",
    "\n",
    "$$f(x + \\varepsilon x') = f(x) + \\varepsilon f'(x). $$\n",
    "\n",
    "For the vector case, we have $P$ numbers $\\varepsilon_i$ and they satisfy $\\varepsilon_i \\varepsilon_j = 0$.\n",
    "\n",
    "If the number of input variables is small, forward mode will be much faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic differentiation (advanced)\n",
    "\n",
    "One can show:\n",
    "- Optimal AD is [NP-complete task](https://link.springer.com/article/10.1007/s10107-006-0042-z)\n",
    "- One can reduce the problem of computing the gradient to the computation of the column of the inverse of the lower-triangular linear system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transposed Jacobian-by-vector for simple layers: linear layer\n",
    "\n",
    "Let $y = f(x, W) = Wx$ be the linear transform. Then the transposed Jacobian times vector for a single sample has the following form:\n",
    "\n",
    "- $\\left(\\frac{\\partial y}{\\partial W}\\right)^{\\top}  o = xo^{\\top},$\n",
    "\n",
    "- $\\left(\\frac{\\partial y}{\\partial x}\\right)^{\\top} o = W^{\\top}o,$\n",
    "\n",
    "where $o = \\frac{\\partial g}{\\partial y}$ comes from backpropagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transposed Jacobian-by-vector for simple layers: nonlinearity\n",
    "\n",
    "Let $y = \\sigma(x)$, then we need to compute\n",
    "\n",
    "$$\n",
    "   \\frac{\\partial y}{\\partial x} o = \\sigma'(x) \\circ o,\n",
    "$$\n",
    "i.e. we need to compute the derivative of $\\sigma$ and do elementwise multiplication with the vector $o$ that comes from backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Storage\n",
    "\n",
    "Consider MLP. Then, computation of the gradient requires:\n",
    "\n",
    "1. Storing all intermediate computation before linear layer and before non-linearity (forward pass).\n",
    "2. Compute transposed Jacobian matrix-by-vector product (backward pass).\n",
    "\n",
    "In inference, we don't need to store intermediate values ($x_k$, also called **activations**).\n",
    "\n",
    "Inference can be initiated as \n",
    "\n",
    "```python \n",
    "with torch.no_grad():\n",
    "    model(x)\n",
    "```\n",
    "\n",
    "But no gradients can be computed in this case! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Building neural network\n",
    "#!pip install -U git+https://github.com/szagoruyko/pytorchviz.git@master\n",
    "import torchviz\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# Define the model architecture\n",
    "    \n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, output_size),)\n",
    "model.to(device)\n",
    "\n",
    "    \n",
    "x = torch.linspace(-2, 2, 256)\n",
    "x = x[:, None]\n",
    "y = x**2\n",
    "\n",
    "def compute_loss(model, x, y):\n",
    "    #with torch.no_grad():\n",
    "    #    er = y - model(x)\n",
    "    #    loss = torch.mean(er**2)\n",
    "    er = y - model(x)\n",
    "    loss = torch.mean(er**2)\n",
    "    \n",
    "    return loss\n",
    "make_dot(compute_loss(model, x, y), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional neural networks (1)\n",
    "\n",
    "For image processing, one of the most efficient architecture is the **convolutional neural network** (CNN)\n",
    "\n",
    "First CNN have been feedforward. \n",
    "\n",
    "The main difference is how the linear layer is implemented. \n",
    "\n",
    "In MLP, we have \n",
    "\n",
    "$$x_{k+1} = \\sigma(W_k x_k + b_k)$$ \n",
    "\n",
    "and $W_k$ is a **dense matrix**. \n",
    "\n",
    "If $x$ is an image, it can be treated a three-dimensional array $X \\times Y \\times S$ (width-height-channels).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional neural networks (2)\n",
    "\n",
    "If we have say $224 \\times 224 \\times 3$ image, we will need to store a $150528 \\times 150528$ matrix, which is impossible.\n",
    "\n",
    "In classical image processing, many transformations are given in the form of **convolutions**.\n",
    "\n",
    "**Reminder**: \n",
    "\n",
    "\n",
    "The most ``important\" and time-consuming operation within modern CNNs is the generalized convolution that maps an input tensor $U(\\cdot, \\cdot, \\cdot)$ of size $X \\times Y \\times S$ into an output tensor $V(\\cdot, \\cdot, \\cdot)$ of size $(X - d + 1) \\times (Y - d + 1) \\times T$ using the following linear mapping:\n",
    "\n",
    "$$V(x, y, t) = \\sum_{i=x-\\delta}^{x+\\delta} \\sum_{j=y-\\delta}^{y+\\delta} \\sum_{s=1}^S K(i - x + \\delta, j - y + \\delta, s, t) U(i, j, s)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN, complexity\n",
    "\n",
    "$$V(x, y, t) = \\sum_{i=x-\\delta}^{x+\\delta} \\sum_{j=y-\\delta}^{y+\\delta} \\sum_{s=1}^S K(i - x + \\delta, j - y + \\delta, s, t) U(i, j, s)$$\n",
    "\n",
    "The size of the filter determines the number of pixels  in the window (here - $\\delta$).\n",
    "\n",
    "If $\\delta$ is small, one can use this formula directly, resulting in $\\delta^2 XY ST$ complexity. No need to store the full layer!\n",
    "\n",
    "If $\\delta$ is large, one can use Fast Fourier Transform (FFT). \n",
    "\n",
    "Nowdays, it is even popular to use **1x1** convolution and depth-wise convolutions (which are not really convolutions).\n",
    "\n",
    "We will discuss these architectures in more detail later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN: the architecture\n",
    "\n",
    "We will discuss CNN tomorrow, including a very important **pooling** layer \n",
    "(which made it possible to learn on the so-called ImageNet dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Popular deep learning libraries \n",
    "\n",
    "\n",
    "The main deep learning libraries available now are **Tensorflow**, **Pytorch** and **Jax**.\n",
    "\n",
    "Lets discuss them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Tensorflow](https://www.tensorflow.org/)\n",
    "\n",
    "- TensorFlow is an open-source platform for building and deploying machine learning models.\n",
    "- Developed by Google and released in 2015.\n",
    "- Provides both high-level APIs for building and training models quickly and low-level APIs for more control and flexibility.\n",
    "- Provides a flexible architecture to support a variety of deployment scenarios, including on-premises, cloud-based, and mobile devices.\n",
    "- TensorFlow has a large and active community, with a vast range of resources and tutorials available online.\n",
    "- The latest version of TensorFlow is TensorFlow 2.16, which has improved ease-of-use and performance compared to previous versions.\n",
    "- TensorFlow also provides tools for visualizing and debugging models, such as TensorBoard.\n",
    "- TensorFlow has support for both CPU and GPU acceleration, allowing for efficient training and inference on a wide range of hardware.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Pytorch](https://pytorch.org/)\n",
    "\n",
    "- Pytorch is an open-source platform for building and deploying machine learning models.\n",
    "- Developed by Facebook and released in 2018.\n",
    "- Dynamic computational graph, easy for debugging and its own memory management\n",
    "- Huge community, the most popular in academia (a lot of pretrained models/code)\n",
    "- Current version is Pytorch 2.0 which is focused on speed (i.e., it has torch.compile)\n",
    "- Now has grad, vmap\n",
    "- CPU/GPU support\n",
    "- Distributed training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Jax](https://github.com/google/jax)\n",
    "\n",
    "- Jax has become quite popular recently, developed by Google in 2018\n",
    "- JAX uses a functional programming model that enables users to express complex numerical computations as pure functions that can be composed, transformed, and optimized for efficient execution. **jax.jit**, **jax.grad**, **jax.vmap** (vectorization), recently similar functionality appeared in Pytorch\n",
    "- JAX is designed to work seamlessly with NumPy, allowing users to take advantage of the vast ecosystem of scientific computing tools available in Python.\n",
    "- JAX is built for performance and scales well on modern hardware, including CPUs, GPUs, and TPUs.\n",
    "- It supports distributed computing and can be used to train large-scale models efficiently.\n",
    "- Quite a few problems if you move from Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison\n",
    "\n",
    "| Tensorflow | Jax | Pytorch |\n",
    "| --- | --- | --- |\n",
    "| + Widely used in industry and academia | + Designed for high-performance numerical computing | + Dynamic computational graph allows for flexibility |\n",
    "| + Large and active community with extensive documentation | + Automatic differentiation for fast and efficient gradient computation | + Supports various neural network architectures |\n",
    "| + Supports distributed computing for large-scale training | + Supports GPU and TPU acceleration | + Easy to use and learn |\n",
    "| - Can have a steep learning curve for beginners | - Smaller community compared to other frameworks | - Some operations can be slower than other frameworks |\n",
    "| - Code can be verbose and difficult to read | - Functional programming knowledge needed | - Debugging can be difficult | \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some hints\n",
    "\n",
    "If the goal is to train a known model on a given dataset, no big difference (Pytorch and Tensorflow have better dataset handling).\n",
    "\n",
    "If we need speed, Jax is a good option.\n",
    "\n",
    "Overall, it is a matter of taste, and the difference between frameworks has become not so evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So, what is deep learning?\n",
    "  \n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <div style=\"flex: 1; margin-right: 20px; margin-top: 20px; width: 50%\">\n",
    "\n",
    "        \n",
    "End-to-end joint learning of all layers:\n",
    "\n",
    "- multiple assemblable blocks (“modules”)\n",
    "- each block is piecewise-differentiable\n",
    "- gradient-based learning  from examples\n",
    "- gradients computed by backpropagation\n",
    "\n",
    "    </div>\n",
    "    <div style=\"flex: 1\">\n",
    "        <img src=\"lego.jpg\" width=\"100%\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take home message\n",
    "\n",
    "- Supervised learning\n",
    "- Basic idea of automatic differentiation (forward/reverse mode)\n",
    "- Fully connected networks\n",
    "- Three main libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next lecture\n",
    "\n",
    "Convolutional neural networks in more details:\n",
    "\n",
    "- Motivation of using convolutions (and connection to classical image processing)\n",
    "- Basic building blocks of a CNN (convolutions in 1D, 2D, 3D, pooling)\n",
    "- Convolutions and (equivariance)\n",
    "- Overview of the main architectures (LeNet, AlexNet, VGG, ResNet, Inception)\n",
    "- Visualization of learned filters\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "nav_menu": {},
  "rise": {
   "scroll": true,
   "theme": "sky",
   "transition": "zoom"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
